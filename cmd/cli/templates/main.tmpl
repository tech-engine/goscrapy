package main

import (
	"context"
	"errors"
	"fmt"
	"sync"
    "os"
	"os/signal"
	"syscall"
	// replace with your own project name
	"{{.}}/{{.}}"

	"github.com/tech-engine/goscrapy/cmd/corespider"
	"github.com/tech-engine/goscrapy/pkg/builtin/pipelines"
)
// sample terminate function to demostrate spider termination.
func OnTerminate(fn func()) {
	ctx, stop := signal.NotifyContext(context.Background(), os.Interrupt, syscall.SIGINT, syscall.SIGTERM)
	<-ctx.Done()
	stop()
	fn()
}

func main() {
	ctx, cancel := context.WithCancel(context.Background())

	var wg sync.WaitGroup
	wg.Add(1)

	// instantiate core spider
	coreSpider := corespider.New[*{{.}}.Record]()

    // we can use middlewares like below
	// coreSpider.MiddlewareManager.Add(
	// 	middlewares.DupeFilter,
	// 	middlewares.MultiCookieJar,
	// )

	// pipelineGroup := pipelinemanager.NewGroup[*{{.}}.Record](
    //  you can add pipelines you want to run concurrenly using pipeline groups
    // )

	// use export 2 csv pipeline
	export2Csv := pipelines.Export2CSV[*{{.}}.Record]()
	export2Csv.WithFilename("itstimeitsnowornever.csv")

	// use export 2 json pipeline
	export2Json := pipelines.Export2JSON[*{{.}}.Record]()
	export2Json.WithImmediate()
	export2Json.WithFilename("itstimeitsnowornever.json")
	
	// we can use piplines
	coreSpider.PipelineManager.Add(
		export2Csv,
		//  pipelineGroup,
		export2Json,
	)

	go func() {
		defer wg.Done()

		err := coreSpider.Start(ctx)

		if err != nil && errors.Is(err, context.Canceled) {
			return
		}

		fmt.Printf("failed: %q", err)
	}()

	spider := {{.}}.NewSpider(coreSpider)

	// start the scraper with a job, currently nil is passed but you can pass your job here
	spider.StartRequest(ctx, nil)

	OnTerminate(func() {
		fmt.Println("exit signal received: shutting down gracefully")
		cancel()
		wg.Wait()
	})

}
